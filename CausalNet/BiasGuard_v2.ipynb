{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BiasGuard: Advanced Bias Mitigation in Large Language Models\n",
        "\n",
        "This notebook demonstrates the key features of the BiasGuard project."
      ],
      "metadata": {
        "id": "hqfHCBK2FXlS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m8Ug4-DFSFM"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from data.data_processing import load_and_process_datasets\n",
        "from models.actor_model import ActorModel\n",
        "from models.critic_model import CriticModel\n",
        "from models.reward_model import RewardModel\n",
        "from training.ppo_trainer import BiasGuardPPOTrainer\n",
        "from training.multi_role_debates import MultiRoleDebateGenerator\n",
        "from training.self_reflection import SelfReflectionModule\n",
        "from evaluation.metrics import compute_perplexity, compute_bleu, compute_diversity\n",
        "from evaluation.bias_evaluation import evaluate_overall_bias, evaluate_bias_categories\n",
        "from utils.config import BiasGuardConfig\n",
        "from utils.visualization import plot_training_progress, plot_bias_categories"
      ],
      "metadata": {
        "id": "EULX0og-FdCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Processing"
      ],
      "metadata": {
        "id": "72_cPN-QFeqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_and_process_datasets()\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Sample data point: {dataset[0]}\")"
      ],
      "metadata": {
        "id": "Mbxwes0HFfkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Initialization"
      ],
      "metadata": {
        "id": "PJxGbyZFFhIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = BiasGuardConfig()\n",
        "actor_model = ActorModel(config.model_id)\n",
        "critic_model = CriticModel(config.model_id)\n",
        "reward_model = RewardModel(config.model_id)"
      ],
      "metadata": {
        "id": "6dNjBKcjFinV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Multi-Role Debate Generation"
      ],
      "metadata": {
        "id": "8BIMpSmlFjqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debate_generator = MultiRoleDebateGenerator()\n",
        "debate_prompts = debate_generator.generate_debate_topics(n=2)\n",
        "for prompt in debate_prompts:\n",
        "    print(prompt)\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "FtEwWqaQFkoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Response and Bias Evaluation"
      ],
      "metadata": {
        "id": "u4nBVlylFle2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = debate_prompts[0]\n",
        "response = actor_model.generate(prompt)\n",
        "bias_score = critic_model.evaluate_bias(response)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Response: {response}\")\n",
        "print(f\"Bias Score: {bias_score}\")"
      ],
      "metadata": {
        "id": "5v8UXuRVFmZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Self-Reflection and Improvement"
      ],
      "metadata": {
        "id": "kvPhTzBgFn1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self_reflection = SelfReflectionModule(actor_model)\n",
        "reflection = self_reflection.reflect_on_response(prompt, response)\n",
        "improved_response = self_reflection.generate_improved_response(prompt, response, reflection)\n",
        "\n",
        "print(f\"Reflection: {reflection}\")\n",
        "print(f\"Improved Response: {improved_response}\")\n",
        "print(f\"Improved Bias Score: {critic_model.evaluate_bias(improved_response)}\")"
      ],
      "metadata": {
        "id": "rwRInU6_Fo_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training Demonstration (Mini-batch)"
      ],
      "metadata": {
        "id": "23sQ39reFp9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BiasGuardPPOTrainer(actor_model, critic_model, reward_model, actor_model.tokenizer)\n",
        "stats, responses, bias_scores, rewards = trainer.train_step(debate_prompts[:5])\n",
        "\n",
        "print(\"Training Stats:\", stats)\n",
        "for i, (response, bias_score, reward) in enumerate(zip(responses, bias_scores, rewards)):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Bias Score: {bias_score}\")\n",
        "    print(f\"Reward: {reward}\")"
      ],
      "metadata": {
        "id": "zKYcXsJtFq5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation Metrics"
      ],
      "metadata": {
        "id": "Jl_xxVv2FsJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = compute_perplexity(actor_model, dataset[:100])\n",
        "bleu_score = compute_bleu(actor_model, dataset[:100])\n",
        "distinct_1, distinct_2 = compute_diversity(actor_model, dataset[:100])\n",
        "overall_bias = evaluate_overall_bias(actor_model, dataset[:100])\n",
        "\n",
        "print(f\"Perplexity: {perplexity}\")\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "print(f\"Distinct-1: {distinct_1}\")\n",
        "print(f\"Distinct-2: {distinct_2}\")\n",
        "print(f\"Overall Bias: {overall_bias}\")"
      ],
      "metadata": {
        "id": "zwG6WPleFtFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Visualization"
      ],
      "metadata": {
        "id": "VMNS9FulFuKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated training progress data\n",
        "import numpy as np\n",
        "\n",
        "steps = list(range(100))\n",
        "metrics = {\n",
        "    'step': steps,\n",
        "    'loss': np.random.rand(100).tolist(),\n",
        "    'perplexity': np.random.rand(100) * 10 + 5,\n",
        "    'bleu_score': np.random.rand(100) * 0.5,\n",
        "    'bias_score': np.random.rand(100) * 0.5\n",
        "}\n",
        "\n",
        "plot_training_progress(metrics)\n",
        "\n",
        "# Simulated bias category data\n",
        "bias_categories = {\n",
        "    'gender': 0.3,\n",
        "    'race': 0.25,\n",
        "    'age': 0.2,\n",
        "    'religion': 0.15,\n",
        "    'nationality': 0.1\n",
        "}\n",
        "\n",
        "plot_bias_categories(bias_categories)"
      ],
      "metadata": {
        "id": "srBhFaufFu4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}