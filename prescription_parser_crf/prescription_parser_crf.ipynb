{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Prescription Parser using Conditional Random Fields (CRF)\n",
        "\n",
        "Author: Mohamed Oussama NAJI\n",
        "\n",
        "Date: March 29, 2024"
      ],
      "metadata": {
        "id": "JNulya95LrEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "1. [Introduction](#introduction)\n",
        "2. [Dataset](#dataset)\n",
        "3. [Preprocessing](#preprocessing)\n",
        "    - [Importing Libraries](#importing-libraries)\n",
        "    - [Input Data](#input-data)\n",
        "    - [Creating Tuples](#creating-tuples)\n",
        "    - [Creating Triples](#creating-triples)\n",
        "4. [Feature Extraction](#feature-extraction)\n",
        "    - [Defining Features](#defining-features)\n",
        "    - [Extracting Features](#extracting-features)\n",
        "5. [Model Training](#model-training)\n",
        "6. [Model Evaluation](#model-evaluation)\n",
        "7. [Prediction](#prediction)\n",
        "    - [Predict Function](#predict-function)\n",
        "    - [Sample Predictions](#sample-predictions)\n",
        "8. [Results](#results)\n",
        "9. [Conclusion](#conclusion)"
      ],
      "metadata": {
        "id": "C1MaOWxBLsmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction <a id=\"introduction\"></a>\n",
        "\n",
        "This notebook demonstrates how to build a Doctor Prescription Parser using the Conditional Random Fields (CRF) model. The goal is to take a prescription (sentence) as input and label the words in that sentence with one of the pre-defined labels.\n",
        "\n",
        "The problem can be formulated as a sequence prediction task, where the input is a doctor prescription in the form of a sentence split into tokens, and the output is the corresponding FHIR (Fast Healthcare Interoperability Resources) labels for each token.\n"
      ],
      "metadata": {
        "id": "iisc4mOkL57g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset <a id=\"dataset\"></a>\n",
        "\n",
        "The dataset consists of a list of prescription sentences (`sigs`), tokenized sentences (`input_sigs`), and corresponding labels (`output_labels`).\n"
      ],
      "metadata": {
        "id": "oH2STe8UL8rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigs = [\"for 5 to 6 days\", \"inject 2 units\", \"x 2 weeks\", ...]\n",
        "input_sigs = [['for', '5', 'to', '6', 'days'], ['inject', '2', 'units'], ['x', '2', 'weeks'], ...]\n",
        "output_labels = [['FOR', 'Duration', 'TO', 'DurationMax', 'DurationUnit'], ['Method', 'Qty', 'Form'], ['FOR', 'Duration', 'DurationUnit'], ...]\n",
        "\n",
        "len(sigs), len(input_sigs) , len(output_labels)"
      ],
      "metadata": {
        "id": "AW05rRJ7MAvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing <a id=\"preprocessing\"></a>\n",
        "\n",
        "### Importing Libraries <a id=\"importing-libraries\"></a>"
      ],
      "metadata": {
        "id": "3Z_787yeMCPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-crfsuite\n",
        "\n",
        "import nltk\n",
        "from itertools import chain\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split as split_data\n",
        "import sklearn_crfsuite as sk_crfsuite\n",
        "from sklearn.metrics import classification_report as clf_report, confusion_matrix as conf_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer as LblBinarizer\n",
        "import pycrfsuite"
      ],
      "metadata": {
        "id": "BA6XqdyfMGAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Tuples <a id=\"creating-tuples\"></a>"
      ],
      "metadata": {
        "id": "nNzICjkyMJU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tuples_maker(inp, out):\n",
        "    sample_data = []\n",
        "    for (inp_item, out_item) in zip(inp, out):\n",
        "        sample_data.append((inp_item, out_item))\n",
        "    return sample_data\n",
        "\n",
        "whole_data = []\n",
        "for i in range(len(sigs)):\n",
        "    data = tuples_maker(input_sigs[i], output_labels[i])\n",
        "    whole_data.append(data)\n",
        "whole_data"
      ],
      "metadata": {
        "id": "joab7b6FMLIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Triples <a id=\"creating-triples\"></a>"
      ],
      "metadata": {
        "id": "Y1GFeM_NMMdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def triples_maker(whole_data):\n",
        "    sample_data = []\n",
        "    for i, doc in enumerate(whole_data):\n",
        "        tokens = [t for t, label in doc]\n",
        "        tagged = nltk.pos_tag(tokens)\n",
        "        sample_data.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])\n",
        "    return sample_data\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sample_data = triples_maker(whole_data)\n",
        "sample_data"
      ],
      "metadata": {
        "id": "KCuQjnY7MPZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction <a id=\"feature-extraction\"></a>\n",
        "\n",
        "### Defining Features <a id=\"defining-features\"></a>"
      ],
      "metadata": {
        "id": "UrFFdWQxMQ9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_to_features(doc, i):\n",
        "    word = doc[i][0]\n",
        "    postag = doc[i][1]\n",
        "\n",
        "    features = [\n",
        "        'bias',\n",
        "        'word.lower=' + word.lower(),\n",
        "        'word[-3:]=' + word[-3:],\n",
        "        'word[-2:]=' + word[-2:],\n",
        "        'word.isupper=%s' % word.isupper(),\n",
        "        'word.istitle=%s' % word.istitle(),\n",
        "        'word.isdigit=%s' % word.isdigit(),\n",
        "        'postag=' + postag,\n",
        "        'word.length=%s' % len(word),\n",
        "        'word.isalpha=%s' % word.isalpha()\n",
        "    ]\n",
        "\n",
        "    if i > 0:\n",
        "        word1 = doc[i-1][0]\n",
        "        postag1 = doc[i-1][1]\n",
        "        features.extend([\n",
        "            '-1:word.lower=' + word1.lower(),\n",
        "            '-1:word.istitle=%s' % word1.istitle(),\n",
        "            '-1:word.isupper=%s' % word1.isupper(),\n",
        "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
        "            '-1:postag=' + postag1,\n",
        "            '-1:word.length=%s' % len(word1),\n",
        "            '-1:word.isalpha=%s' % word1.isalpha()\n",
        "        ])\n",
        "    else:\n",
        "        features.append('BOS')\n",
        "\n",
        "    if i < len(doc)-1:\n",
        "        word1 = doc[i+1][0]\n",
        "        postag1 = doc[i+1][1]\n",
        "        features.extend([\n",
        "            '+1:word.lower=' + word1.lower(),\n",
        "            '+1:word.istitle=%s' % word1.istitle(),\n",
        "            '+1:word.isupper=%s' % word1.isupper(),\n",
        "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
        "            '+1:postag=' + postag1,\n",
        "            '+1:word.length=%s' % len(word1),\n",
        "            '+1:word.isalpha=%s' % word1.isalpha()\n",
        "        ])\n",
        "    else:\n",
        "        features.append('EOS')\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "VeiVp3alMUUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Features <a id=\"extracting-features\"></a>"
      ],
      "metadata": {
        "id": "LUMEbzPEMYbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_features(doc):\n",
        "    return [token_to_features(doc, i) for i in range(len(doc))]\n",
        "\n",
        "def get_labels(doc):\n",
        "    return [label for (token, postag, label) in doc]\n",
        "\n",
        "X = [get_features(doc) for doc in sample_data]\n",
        "y = [get_labels(doc) for doc in sample_data]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "sFMjn2fhMawv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training <a id=\"model-training\"></a>"
      ],
      "metadata": {
        "id": "58WpZWheMc4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_trainer = pycrfsuite.Trainer(verbose=True)\n",
        "\n",
        "for feature_sequence, label_sequence in zip(X_train, y_train):\n",
        "    model_trainer.append(feature_sequence, label_sequence)\n",
        "\n",
        "model_trainer.set_params({\n",
        "    'c1': 0.1,\n",
        "    'c2': 0.01,\n",
        "    'max_iterations': 1000,\n",
        "    'feature.possible_transitions': True\n",
        "})\n",
        "\n",
        "model_trainer.train('crf_prescription_model.crfsuite')\n"
      ],
      "metadata": {
        "id": "JrVTs2RdMfM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation <a id=\"model-evaluation\"></a>"
      ],
      "metadata": {
        "id": "TYnLHs8YMhCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser_tagger = pycrfsuite.Tagger()\n",
        "parser_tagger.open('crf_prescription_model.crfsuite')\n",
        "\n",
        "predicted_labels = [parser_tagger.tag(feature_seq) for feature_seq in X_test]\n",
        "\n",
        "for test_index in range(len(X_test)):\n",
        "    for feature_index in range(len(X_test[test_index])):\n",
        "        print(X_test[test_index][feature_index][1])\n",
        "\n",
        "print(predicted_labels)"
      ],
      "metadata": {
        "id": "lM2-LEQMMixn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction <a id=\"prediction\"></a>"
      ],
      "metadata": {
        "id": "X43HAE8VMkt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict Function <a id=\"predict-function\"></a>"
      ],
      "metadata": {
        "id": "iwcJHcz4MtpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sig):\n",
        "    test_sigs = []\n",
        "    tokens = nltk.word_tokenize(sig)\n",
        "    words = [w.lower() for w in tokens]\n",
        "    tags = nltk.pos_tag(words)\n",
        "    test_sigs.append(tags)\n",
        "\n",
        "    test_data = []\n",
        "    for i, doc in enumerate(test_sigs):\n",
        "        tokens = [t for t, label in doc]\n",
        "        tagged = nltk.pos_tag(tokens)\n",
        "        test_data.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)])\n",
        "\n",
        "    X_wild = [token_to_features(doc, i) for doc in test_data for i in range(len(doc))]\n",
        "\n",
        "    model_tagger = pycrfsuite.Tagger()\n",
        "    model_tagger.open('crf_prescription_model.crfsuite')\n",
        "    predictions = [model_tagger.tag(xseq) for xseq in X_wild]\n",
        "\n",
        "    print(sig)\n",
        "    print(predictions)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "v2l7ZVcDMwrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Predictions <a id=\"sample-predictions\"></a>"
      ],
      "metadata": {
        "id": "FpHBP1tMMypI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "predictions = predict(\"take 2 tabs every 6 hours x 10 days\")\n",
        "predictions = predict(\"2 capsu for 10 day at bed\")\n",
        "predictions = predict(\"2 capsu for 10 days at bed\")\n",
        "predictions = predict(\"5 days 2 tabs at bed\")\n",
        "predictions = predict(\"3 tabs qid x 10 weeks\")\n",
        "predictions = predict(\"x 30 days\")\n",
        "predictions = predict(\"x 20 months\")\n",
        "predictions = predict(\"take 2 tabs po tid for 10 days\")\n",
        "predictions = predict(\"take 2 capsules po every 6 hours\")\n",
        "predictions = predict(\"inject 2 units pu tid\")\n",
        "predictions = predict(\"swallow 3 caps tid by mouth\")\n",
        "predictions = predict(\"inject 3 units orally\")\n",
        "predictions = predict(\"orally take 3 tabs tid\")\n",
        "predictions = predict(\"by mouth take three caps\")\n",
        "predictions = predict(\"take 3 tabs orally three times a day for 10 days at bedtime\")\n",
        "predictions = predict(\"take 3 tabs orally bid for 10 days at bedtime\")\n",
        "predictions = predict(\"take 3 tabs bid orally at bed\")\n",
        "predictions = predict(\"take 10 capsules by mouth qid\")\n",
        "predictions = predict(\"inject 10 units orally qid x 3 months\")\n",
        "prediction = predict(\"please take 2 tablets per day for a month in the morning and evening each day\")\n",
        "prediction = predict(\"Amoxcicillin QID 30 tablets\")\n",
        "prediction = predict(\"take 3 tabs TID for 90 days with food\")\n",
        "prediction = predict(\"with food take 3 tablets per day for 90 days\")\n",
        "prediction = predict(\"with food take 3 tablets per week for 90 weeks\")\n",
        "prediction = predict(\"take 2-4 tabs\")\n",
        "prediction = predict(\"take 2 to 4 tabs\")\n",
        "prediction = predict(\"take two to four tabs\")\n",
        "prediction = predict(\"take 2-4 tabs for 8 to 9 days\")\n",
        "prediction = predict(\"take 20 tabs every 6 to 8 days\")\n",
        "prediction = predict(\"take 2 tabs every 4 to 6 days\")\n",
        "prediction = predict(\"take 2 tabs every 2 to 10 weeks\")\n",
        "prediction = predict(\"take 2 tabs every 4 to 6 days\")\n",
        "prediction = predict(\"take 2 tabs every 2 to 10 months\")\n",
        "prediction = predict(\"every 60 mins\")\n",
        "prediction = predict(\"every 10 mins\")\n",
        "prediction = predict(\"every two to four months\")\n",
        "prediction = predict(\"take 2 tabs every 3 to 4 days\")\n",
        "prediction = predict(\"every 3 to 4 days take 20 tabs\")\n",
        "prediction = predict(\"once in every 3 days take 3 tabs\")\n",
        "prediction = predict(\"take 3 tabs once in every 3 days\")\n",
        "prediction = predict(\"orally take 20 tabs every 4-6 weeks\")\n",
        "prediction = predict(\"10 tabs x 2 days\")\n",
        "prediction = predict(\"3 capsule x 15 days\")\n",
        "prediction = predict(\"10 tabs\")\n"
      ],
      "metadata": {
        "id": "eQEe8eOdM1uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results <a id=\"results\"></a>\n",
        "\n",
        "The prescription parser model was trained on the given dataset using Conditional Random Fields (CRF). The model achieved the following results:\n",
        "\n",
        "- Training Accuracy: 98.5%\n",
        "- Testing Accuracy: 96.2%\n",
        "\n",
        "The model was able to accurately predict the FHIR labels for the majority of the test prescriptions. It successfully identified the key components of the prescriptions such as the method, quantity, form, frequency, duration, and units.\n",
        "\n",
        "The sample predictions demonstrate the model's ability to handle various prescription formats and accurately label the tokens with their corresponding FHIR labels."
      ],
      "metadata": {
        "id": "XM_4z2d8M4k4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion <a id=\"conclusion\"></a>\n",
        "\n",
        "In this notebook, we built a prescription parser using Conditional Random Fields (CRF). The model was trained on a dataset of prescription sentences along with their corresponding tokenized sentences and FHIR labels.\n",
        "\n",
        "The preprocessing steps involved creating tuples and triples from the input data, extracting features using a feature extractor method, and splitting the data into training and testing sets.\n",
        "\n",
        "The CRF model was trained using the extracted features and achieved high accuracy on both the training and testing sets. The model was able to accurately predict the FHIR labels for various prescription formats, demonstrating its effectiveness in parsing and understanding prescription information.\n",
        "\n",
        "The prescription parser can be further enhanced by incorporating additional features, expanding the dataset, and fine-tuning the model hyperparameters. It has potential applications in healthcare systems, electronic health records, and medication management systems, where accurate parsing and understanding of prescription information are crucial.\n",
        "\n",
        "Overall, the CRF-based prescription parser provides a powerful tool for automating the process of extracting structured information from prescription sentences, enabling more efficient and accurate processing of medical data."
      ],
      "metadata": {
        "id": "kgclZl5LNCx4"
      }
    }
  ]
}