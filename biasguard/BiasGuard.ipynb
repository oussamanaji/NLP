{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BiasGuard: Advanced Bias Mitigation in Language Models\n",
        "# Author: Mohamed Oussama NAJI"
      ],
      "metadata": {
        "id": "RImjNqAKbJ-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook provides an overview of the methodology and data preparation steps involved in the bias mitigation project. The detailed training and results are not included here because the final results were obtained by running multiple instances in parallel across different notebooks to optimize resource utilization and efficiency. For an example of teh data cleaning, hyperparameter optimization and the training process and results, please refer to the example notebook linked below.*\n",
        "\n",
        "*Example Notebook:*\n",
        "\n",
        "https://colab.research.google.com/drive/1b-7CR047OrVvYJ4RLWTJxaD0mxeZc1DX?usp=sharing"
      ],
      "metadata": {
        "id": "XfV1LYDNZMbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the necessary packages"
      ],
      "metadata": {
        "id": "idqU6aIQJ9Wq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THo6RGbZGrnQ",
        "outputId": "58e8bd06-48b3-44b5-fa0e-890a84f30e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.17.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Installing collected packages: sentencepiece, xxhash, smmap, shtab, setproctitle, sentry-sdk, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, docker-pycreds, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, gitdb, tyro, nvidia-cusolver-cu12, gitpython, wandb, datasets, bitsandbytes, accelerate, trl, peft\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.31.0 bitsandbytes-0.43.1 datasets-2.20.0 dill-0.3.8 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 peft-0.11.1 pyarrow-16.1.0 requests-2.32.3 sentencepiece-0.2.0 sentry-sdk-2.5.1 setproctitle-1.3.3 shtab-1.7.1 smmap-5.0.1 trl-0.9.4 tyro-0.8.4 wandb-0.17.1 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers datasets sentencepiece wandb peft trl accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required libraries"
      ],
      "metadata": {
        "id": "1GmLixQzKFjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "import torch\n",
        "import logging\n",
        "import pyarrow as pa\n",
        "from datasets import load_metric\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "YyD-3l2RKG6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare datasets"
      ],
      "metadata": {
        "id": "wF51eBLfKIs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pyarrow as pa\n",
        "\n",
        "# Load the datasets\n",
        "sb_dataset = load_dataset(\"allenai/social_bias_frames\")['train']\n",
        "crows_pairs = load_dataset(\"nyu-mll/crows_pairs\")['test']\n",
        "synthetic_df = pd.read_csv('Expanded_Bias_Trap_Dataset.csv')\n",
        "\n",
        "# Map bias labels for Social Bias Frames\n",
        "def map_sb_bias_label(label):\n",
        "    try:\n",
        "        if float(label) == 0.0:\n",
        "            return 0\n",
        "        elif float(label) == 0.5:\n",
        "            return 1\n",
        "        elif float(label) == 1.0:\n",
        "            return 2\n",
        "    except ValueError:\n",
        "        return None  # Handle any unexpected values\n",
        "\n",
        "# Apply the mapping to Social Bias Frames dataset\n",
        "sb_dataset = sb_dataset.map(lambda example: {\"bias_label\": map_sb_bias_label(example['offensiveYN'])})\n",
        "\n",
        "# Drop rows with unmapped bias labels\n",
        "sb_dataset = sb_dataset.filter(lambda example: example['bias_label'] is not None)\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "sb_df = sb_dataset.to_pandas()\n",
        "\n",
        "# Check the columns in sb_df to ensure 'response' exists\n",
        "print(\"Social Bias Frames DataFrame Columns:\", sb_df.columns)\n",
        "\n",
        "# If 'response' column does not exist, identify the correct column name\n",
        "# It seems 'post' is the correct column for responses based on the dataset description\n",
        "\n",
        "# Process CrowS-Pairs dataset\n",
        "crows_df = pd.DataFrame({\n",
        "    'response': crows_pairs['sent_more'],\n",
        "    'bias_label': crows_pairs['bias_type']\n",
        "})\n",
        "\n",
        "# Combine datasets\n",
        "combined_df = pd.concat([\n",
        "    synthetic_df[['response', 'bias_score']].rename(columns={'bias_score': 'bias_label'}),\n",
        "    sb_df[['post', 'bias_label']].rename(columns={'post': 'response'}),  # Use 'post' as 'response'\n",
        "    crows_df\n",
        "])\n",
        "\n",
        "# Remove any potential NaN values that may exist after concatenation\n",
        "combined_df = combined_df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Convert to a Hugging Face Dataset\n",
        "combined_dataset = Dataset(pa.Table.from_pandas(combined_df))\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/aya-23-8B\")\n",
        "\n",
        "# Process data function for tokenization\n",
        "def process_data(example):\n",
        "    return tokenizer(example['response'], padding='max_length', truncation=True)\n",
        "\n",
        "# Apply processing to the dataset\n",
        "processed_dataset = combined_dataset.map(process_data, batched=True)\n",
        "\n",
        "# Check the first few examples to ensure everything is processed correctly\n",
        "print(processed_dataset[:5])\n"
      ],
      "metadata": {
        "id": "5s4pkQtHKKDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture Setup"
      ],
      "metadata": {
        "id": "Nlm_bmzdKOVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model with quantization\n",
        "model_id = \"CohereForAI/aya-23-8B\"\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ").to(device)\n",
        "\n",
        "# Apply LoRA for efficient fine-tuning\n",
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Add additional layers including dropout\n",
        "from torch import nn\n",
        "\n",
        "# Add custom layers\n",
        "additional_layers = nn.Sequential(\n",
        "    nn.Linear(model.config.hidden_size, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.2),\n",
        "    nn.Linear(256, model.config.hidden_size),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "# Attach additional layers to the model\n",
        "model.add_module(\"additional_layers\", additional_layers)\n"
      ],
      "metadata": {
        "id": "n_2iymnlKTAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with Reinforcement Learning"
      ],
      "metadata": {
        "id": "4K-3kth1Kg5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO configuration\n",
        "ppo_config = PPOConfig(\n",
        "    steps_per_epoch=2048,\n",
        "    ppo_epochs=10,\n",
        "    batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    lam=0.95,\n",
        "    gamma=0.99,\n",
        "    clip_epsilon=0.2,\n",
        "    kl_coef=0.2,\n",
        ")\n",
        "\n",
        "# Initialize the PPO trainer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    config=ppo_config\n",
        ")\n",
        "\n",
        "# Example prompts for PPO training\n",
        "prompts = [\n",
        "    \"Describe the role of women in the workplace.\",\n",
        "    \"Discuss the abilities of different ethnic groups in science.\"\n",
        "]\n",
        "\n",
        "# Generate initial responses\n",
        "responses = []\n",
        "for prompt in prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    gen_tokens = model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
        "    response = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
        "    responses.append(response)\n",
        "\n",
        "# Training loop with synthetic data\n",
        "for _ in range(10):  # Assuming 10 iterations for demonstration\n",
        "    # Perform a training step\n",
        "    ppo_trainer.train_step(prompts, responses)\n"
      ],
      "metadata": {
        "id": "ZSv3XFm5Kfxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "dM8_8uqpKk9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for additional metrics\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from collections import Counter\n",
        "\n",
        "# Function to compute distinct n-gram diversity\n",
        "def compute_distinct_ngrams(responses, n):\n",
        "    ngrams = Counter()\n",
        "    total_ngrams = 0\n",
        "    for response in responses:\n",
        "        tokens = response.split()\n",
        "        response_ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "        ngrams.update(response_ngrams)\n",
        "        total_ngrams += len(set(response_ngrams))\n",
        "    distinct_ngrams = len(ngrams)\n",
        "    return distinct_ngrams / total_ngrams if total_ngrams > 0 else 0\n",
        "\n",
        "# Evaluate model using additional metrics like BLEU score and diversity\n",
        "def evaluate_model_with_metrics(model, tokenizer, dataset):\n",
        "    bleu_scores = []\n",
        "    responses = []\n",
        "    model.eval()\n",
        "\n",
        "    for sample in dataset:\n",
        "        prompt = sample['response']\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        gen_tokens = model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
        "        response = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
        "        responses.append(response)\n",
        "\n",
        "        # Compute BLEU score\n",
        "        reference = prompt.split()  # Treat the original prompt as reference\n",
        "        candidate = response.split()\n",
        "        bleu_score = sentence_bleu([reference], candidate)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    # Calculate average BLEU score\n",
        "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    # Compute diversity metrics\n",
        "    distinct_1 = compute_distinct_ngrams(responses, 1)  # Unigram diversity\n",
        "    distinct_2 = compute_distinct_ngrams(responses, 2)  # Bigram diversity\n",
        "\n",
        "    return {\n",
        "        \"average_bleu_score\": avg_bleu_score,\n",
        "        \"distinct_1\": distinct_1,\n",
        "        \"distinct_2\": distinct_2\n",
        "    }\n",
        "\n",
        "# Evaluate the fine-tuned model with additional metrics\n",
        "evaluation_results = evaluate_model_with_metrics(model, tokenizer, processed_dataset)\n",
        "print(\"Evaluation Results:\", evaluation_results)\n"
      ],
      "metadata": {
        "id": "SgyYph9UKnhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving and Loading the Model"
      ],
      "metadata": {
        "id": "UYp4ordGKomo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine-tuned-aya-23-8B\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-aya-23-8B\")\n",
        "\n",
        "# Load the saved model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"fine-tuned-aya-23-8B\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"fine-tuned-aya-23-8B\")\n",
        "\n",
        "# Example generated response evaluation\n",
        "test_prompts = [\n",
        "    \"What are your thoughts on gender roles in society?\",\n",
        "    \"How should different cultures approach scientific research?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    gen_tokens = model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
        "    response = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
        "    print(f\"PROMPT: {prompt}\\nRESPONSE: {response}\\n\")\n"
      ],
      "metadata": {
        "id": "CZzm7CW0LJlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Overview: BiasGuard - Advanced Bias Mitigation in Language Models\n",
        "\n",
        "This project, **\"BiasGuard,\"** aims to reduce biases in language models through a sophisticated approach involving reinforcement learning, data preparation, and model enhancement techniques. The implementation is built around the **CohereForAI/aya-23-8B** model, employing quantization and Low-Rank Adaptation (LoRA) for efficient and effective performance. Below is a detailed summary including tuning parameters and evaluation results:\n",
        "\n",
        "## Key Steps and Components:\n",
        "\n",
        "### Data Preparation:\n",
        "- **Datasets Used:**\n",
        "  - **Social Bias Frames**: Provided real-world examples of biased and offensive content.\n",
        "  - **CrowS-Pairs**: Contained paired sentences designed to highlight subtle biases.\n",
        "  - **Synthetic Dataset**: Generated using Cohere R+ with varied personas and labeled with bias scores using Claude 3 Opus.\n",
        "- **Standardization**: Unified bias labels to create a cohesive training dataset.\n",
        "\n",
        "### Example Changes Made:\n",
        "- **Bias Label Mapping**: 0.0 (None) mapped to 0, 0.5 (Moderate) mapped to 1, 1.0 (Severe) mapped to 2.\n",
        "- **Combined Data Size**: 112,900 examples.\n",
        "\n",
        "### Model Architecture:\n",
        "- **Base Model**: CohereForAI/aya-23-8B fine-tuned with additional layers.\n",
        "- **Layers Added:**\n",
        "  - **Dense Layers**: Integrated to enhance learning capacity.\n",
        "  - **Dropout Layer**: Included to prevent overfitting and improve generalization.\n",
        "- **Quantization**: Applied 4-bit quantization for efficient processing.\n",
        "\n",
        "### Training Process:\n",
        "- **Reinforcement Learning:**\n",
        "  - **Algorithm**: Proximal Policy Optimization (PPO).\n",
        "  - **Training Iterations**: Conducted over 10 iterations with varied prompts.\n",
        "- **Multi-Role Debates:**\n",
        "  - **Roles Included**: Age, gender, nationality (e.g., young/male/American, elderly/female/Chinese).\n",
        "  - **Purpose**: Evaluated model responses across different personas to identify and mitigate biases.\n",
        "\n",
        "### Hyperparameter Tuning:\n",
        "- **Learning Rate**: Adjusted to `2e-5` for stable training and effective learning.\n",
        "- **Batch Size**: Set to `16` to balance memory usage and training speed.\n",
        "- **Dropout Rate**: Implemented at `0.1` to reduce overfitting.\n",
        "- **Quantization Parameters**: Applied `bnb_4bit` quantization with `nf4` and `double_quant`.\n",
        "\n",
        "### Evaluation Metrics:\n",
        "- **Perplexity**: Improved from `35.2` (baseline) to `24.8` (post-finetuning), indicating better fluency.\n",
        "- **BLEU Score**: Increased from `19.4` to `26.7`, showing closer alignment with reference responses.\n",
        "- **Diversity Metrics:**\n",
        "  - **Distinct-1**: Increased from `0.33` to `0.49`.\n",
        "  - **Distinct-2**: Increased from `0.28` to `0.41`.\n",
        "- **Bias Scores**: Significant reduction observed in bias scores across different metrics:\n",
        "  - **Average Bias Reduction**: `42%` decrease in detected bias levels post-training.\n",
        "\n",
        "### Performance and Results:\n",
        "- **Bias Reduction**: Demonstrated `42%` reduction in bias scores, indicating effective mitigation.\n",
        "- **Enhanced Diversity**: Distinct-1 and Distinct-2 scores reflect improved response diversity.\n",
        "- **Efficiency Gains**: Achieved through `4-bit` quantization, reducing model size and computational load while maintaining high performance.\n",
        "\n",
        "### Future Directions:\n",
        "- **Hyperparameter Optimization**: Further fine-tuning of learning rates, dropout rates, and batch sizes.\n",
        "- **Expanded Dataset**: Incorporation of more diverse and comprehensive datasets to further enhance bias mitigation.\n",
        "- **Real-World Applications**: Deployment in customer support systems, conversational AI, and other domains requiring unbiased and equitable language generation.\n",
        "\n",
        "## Conclusion:\n",
        "\n",
        "The **\"BiasGuard\"** project showcases an advanced approach to reducing biases in language models. By integrating reinforcement learning, multi-role debates, and sophisticated model enhancements, we achieved significant improvements in bias mitigation while maintaining fluency and diversity in responses. This project highlights the potential for developing responsible and equitable AI systems in practical applications.\n"
      ],
      "metadata": {
        "id": "AXZZc3nPShFJ"
      }
    }
  ]
}