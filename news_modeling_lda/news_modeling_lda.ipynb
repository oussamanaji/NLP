{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# News Modeling with Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "Author: Mohamed Oussama Naji\n",
        "\n",
        "Date: March 26, 2024"
      ],
      "metadata": {
        "id": "dVF1OzstpX8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will perform topic modeling on a news article discussing neural networks using Latent Dirichlet Allocation (LDA). The goal is to extract the main themes or topics from the article and gain insights into the content. We will compare several summarization techniques, including Gensim, Summa, TextRank, LexRank, Luhn, LSA, BART, PEGASUS, and T5, to determine which method produces the most coherent and informative summary."
      ],
      "metadata": {
        "id": "n4bLhxyDoWnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "1. [Installation and Setup](#installation-setup)\n",
        "2. [Importing Libraries](#importing-libraries)\n",
        "3. [Text Extraction](#text-extraction)\n",
        "4. [Summarization with Transformers](#summarization-transformers)\n",
        "5. [Summarization with Sumy](#summarization-sumy)\n",
        "6. [Execution and Results](#execution-results)\n",
        "7. [Conclusion](#conclusion)"
      ],
      "metadata": {
        "id": "ltpgECg_oqZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation and Setup <a id=\"installation-setup\"></a>\n",
        "\n",
        "Installing the necessary libraries and modifying the relevant packages."
      ],
      "metadata": {
        "id": "VcaQSkhqotu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n",
        "!pip install gensim==3.6.0\n",
        "!pip install summa\n",
        "!sed -i 's/from collections import Mapping/from collections.abc import Mapping/g' /usr/local/lib/python3.10/dist-packages/gensim/corpora/dictionary.py\n",
        "!sed -i 's/from collections.abc import Mapping, defaultdict/from collections.abc import Mapping\\nfrom collections import defaultdict/g' /usr/local/lib/python3.10/dist-packages/gensim/corpora/dictionary.py\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "C8U13yGxovjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries <a id=\"importing-libraries\"></a>"
      ],
      "metadata": {
        "id": "4EMeQ9PQoxNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.summarization import summarize as gensim_summarize\n",
        "from summa import summarizer as summa_summarizer\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, PegasusForConditionalGeneration, PegasusTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "import nltk\n",
        "from sumy.parsers.html import HtmlParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer"
      ],
      "metadata": {
        "id": "FrlXnWQ3ozJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Extraction <a id=\"text-extraction\"></a>\n",
        "\n",
        "Function to extract the text using a user agent and BeautifulSoup."
      ],
      "metadata": {
        "id": "E0SJjL4Wo0l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_medium(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        article_text = ' '.join([p.text for p in soup.find_all('p')])\n",
        "        return article_text\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "JeJmYGO0o43e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization with Transformers <a id=\"summarization-transformers\"></a>\n",
        "\n",
        "Summarization function for transformer models."
      ],
      "metadata": {
        "id": "9b-Rtcifo7AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_with_transformers(text, model, tokenizer, max_length=1024, min_length=40):\n",
        "    model_max_length = tokenizer.model_max_length\n",
        "    max_length = min(max_length, model_max_length)\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "twOT2VkJo8t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization with Sumy <a id=\"summarization-sumy\"></a>\n",
        "\n",
        "Sumy summarization function."
      ],
      "metadata": {
        "id": "D6FnCvyfo-kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_with_sumy(url):\n",
        "    parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
        "    doc = parser.document\n",
        "\n",
        "    summarizers = [TextRankSummarizer(), LexRankSummarizer(), LuhnSummarizer(), LsaSummarizer()]\n",
        "    summaries = {}\n",
        "    for summarizer in summarizers:\n",
        "        summary = summarizer(doc, 5)\n",
        "        summaries[summarizer.__class__.__name__] = ' '.join([sentence.__str__() for sentence in summary])\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "2tEWiNlppAxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution and Results <a id=\"execution-results\"></a>\n",
        "\n",
        "Defining the URL and extracting the text."
      ],
      "metadata": {
        "id": "fJcBZm0xpFWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://medium.com/@subashgandyer/papa-what-is-a-neural-network-c5e5cc427c7\"\n",
        "\n",
        "article_text = extract_text_from_medium(url)"
      ],
      "metadata": {
        "id": "ITirzbOtpJGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing summarizations."
      ],
      "metadata": {
        "id": "9GXgmlwipKYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if article_text:\n",
        "    # Gensim summarization\n",
        "    print(\"Gensim Summary:\", gensim_summarize(article_text, word_count=200))\n",
        "    print()\n",
        "\n",
        "    # Summa summarization\n",
        "    print(\"Summa Summary:\", summa_summarizer.summarize(article_text, ratio=0.1))\n",
        "    print()\n",
        "\n",
        "    # Sumy summarizations\n",
        "    sumy_summaries = summarize_with_sumy(url)\n",
        "    for name, summary in sumy_summaries.items():\n",
        "        print(f\"{name} Summary:\", summary)\n",
        "        print()\n",
        "\n",
        "    # BART summarization\n",
        "    bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "    bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "    print(\"BART Summary:\", summarize_with_transformers(article_text, bart_model, bart_tokenizer))\n",
        "    print()\n",
        "\n",
        "    # PEGASUS summarization\n",
        "    pegasus_model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
        "    pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "    print(\"PEGASUS Summary:\", summarize_with_transformers(article_text, pegasus_model, pegasus_tokenizer))\n",
        "    print()\n",
        "\n",
        "    # T5 summarization\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "    print(\"T5 Summary:\", summarize_with_transformers(article_text, t5_model, t5_tokenizer))\n",
        "    print()\n",
        "else:\n",
        "    print(\"No content was extracted for summarization.\")\n"
      ],
      "metadata": {
        "id": "N7fSTAkvpNIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion <a id=\"conclusion\"></a>\n",
        "\n",
        "In this notebook, we performed topic modeling on a news article discussing neural networks using Latent Dirichlet Allocation (LDA). We explored various summarization techniques, including Gensim, Summa, TextRank, LexRank, Luhn, LSA, BART, PEGASUS, and T5, to generate concise summaries of the article.\n",
        "\n",
        "The summarization results demonstrated that each technique captured different aspects of the article and provided varying levels of coherence and informativeness. The transformer-based models (BART, PEGASUS, and T5) generally produced more fluent and contextually relevant summaries compared to the traditional extractive methods.\n",
        "\n",
        "However, the effectiveness of each summarization technique may depend on the specific article content, length, and desired level of abstraction. It is important to evaluate and compare multiple techniques to identify the most suitable approach for a given text summarization task.\n",
        "\n",
        "This analysis showcases the power of topic modeling and summarization techniques in extracting key themes and generating concise summaries from news articles. These techniques can be further refined and adapted to suit specific requirements and domains.\n",
        "\n",
        "For future work, we can explore additional preprocessing techniques, fine-tune the summarization models on domain-specific data, and incorporate user feedback to improve the quality and relevance of the generated summaries."
      ],
      "metadata": {
        "id": "j_TErG-0pP9E"
      }
    }
  ]
}
