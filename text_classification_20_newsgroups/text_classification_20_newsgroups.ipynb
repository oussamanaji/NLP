{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MDbRSVYpQ1p"
      },
      "source": [
        "#**Text Classification using 20 Newsgroups Dataset**\n",
        "\n",
        "Author: Mohamed Oussama NAJI\n",
        "\n",
        "Date: March 13, 2024\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we will explore text classification using the 20 Newsgroups dataset. We will use various feature extraction techniques such as CountVectorizer, TfidfVectorizer, Word2Vec, and Doc2Vec, along with different classifiers like MultinomialNB, LogisticRegression, SVC, and DecisionTreeClassifier. We will perform a grid search to find the best combination of vectorizer and classifier parameters.\n",
        "\n",
        "## Table of Contents\n",
        "1. Importing Libraries\n",
        "2. Loading the Dataset\n",
        "3. Custom Transformers\n",
        "   - Word2VecTransformer\n",
        "   - Doc2VecTransformer\n",
        "4. Defining the Pipeline\n",
        "5. Defining Grid Search Parameters\n",
        "6. Executing Grid Search\n",
        "7. Saving and Printing Results\n",
        "8. Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries <a id=\"importing-libraries\"></a>"
      ],
      "metadata": {
        "id": "zy_qKOXZ4X-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1vto29jpQ1u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9oCFDNopQ1v"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO6lyiqxpQ1w"
      },
      "source": [
        "## Loading the Dataset <a id=\"loading-dataset\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAZvGwLmpQ1w"
      },
      "outputs": [],
      "source": [
        "categories = ['alt.atheism', 'talk.religion.misc']\n",
        "data = fetch_20newsgroups(subset='train', categories=categories)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbLSjF5IpQ1w"
      },
      "source": [
        "## Custom Transformers <a id=\"custom-transformers\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2VecTransformer <a id=\"word2vec-transformer\"></a>\n"
      ],
      "metadata": {
        "id": "92_KscXq413M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LRGfQj6pQ1x"
      },
      "outputs": [],
      "source": [
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size=100, min_count=1):\n",
        "        self.size = size\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [word_tokenize(doc) for doc in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.size, min_count=self.min_count)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.model.wv[word] for word in words if word in self.model.wv]\n",
        "                    or [np.zeros(self.size)], axis=0) for words in X])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WHltzR-pQ1y"
      },
      "source": [
        "### Doc2VecTransformer <a id=\"doc2vec-transformer\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cq7-4b4pQ1y"
      },
      "outputs": [],
      "source": [
        "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, min_count=1, epochs=40):\n",
        "        self.vector_size = vector_size\n",
        "        self.min_count = min_count\n",
        "        self.epochs = epochs\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(X)]\n",
        "        self.model = Doc2Vec(tagged_data, vector_size=self.vector_size, min_count=self.min_count, epochs=self.epochs)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([self.model.infer_vector(word_tokenize(doc)) for doc in X])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LPNcl6xpQ1z"
      },
      "source": [
        "## Defining the Pipeline <a id=\"defining-pipeline\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-AjnR-WpQ11"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),  # Placeholder vectorizer\n",
        "    ('clf', LogisticRegression()),  # Placeholder classifier\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKvUL9_DpQ11"
      },
      "source": [
        "## Defining Grid Search Parameters <a id=\"defining-grid-search-parameters\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJGJwTOepQ12"
      },
      "outputs": [],
      "source": [
        "parameters = [\n",
        "    {\n",
        "        'vect': [CountVectorizer(), TfidfVectorizer()],\n",
        "        'vect__ngram_range': [(1, 1), (1, 2)],  # Test both unigrams and bigrams\n",
        "        'clf': [MultinomialNB(), LogisticRegression(), SVC(), DecisionTreeClassifier()],\n",
        "    },\n",
        "    {\n",
        "        'vect': [Word2VecTransformer(), Doc2VecTransformer()],\n",
        "        'clf': [LogisticRegression(), SVC(), DecisionTreeClassifier()],  # MultinomialNB is excluded\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXecUF7BpQ13"
      },
      "source": [
        "## Executing Grid Search <a id=\"executing-grid-search\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I1ojAAgpQ13"
      },
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(data.data, data.target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg0eTEz4pQ14"
      },
      "source": [
        "## Saving and Printing Results <a id=\"saving-printing-results\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCCCRM1tpQ14"
      },
      "outputs": [],
      "source": [
        "def save_and_print_results(grid_search):\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    selected_columns = ['rank_test_score', 'mean_test_score', 'std_test_score', 'param_vect', 'param_vect__ngram_range', 'param_clf']\n",
        "    results_df = results_df[selected_columns].copy()\n",
        "    results_df.columns = ['Rank', 'Mean Test Score', 'Std Test Score', 'Vectorizer', 'N-Gram Range', 'Classifier']\n",
        "\n",
        "    tabular_data = results_df.to_string(index=False)\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "    best_vect = best_params.get('vect', 'Vectorizer not specified')\n",
        "    best_clf = best_params.get('clf', 'Classifier not specified')\n",
        "\n",
        "    best_score = f\"\\nBest score: {grid_search.best_score_:.3f}\"\n",
        "    best_params_str = f\"Best parameters set: Vectorizer: {best_vect}, Classifier: {best_clf}\"\n",
        "    tabular_data += best_score + '\\n' + best_params_str\n",
        "\n",
        "    with open('Oussama_Task0_Text_Classification.txt', 'w') as f:\n",
        "        f.write(tabular_data)\n",
        "\n",
        "    print(tabular_data)\n",
        "\n",
        "save_and_print_results(grid_search)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion <a id=\"conclusion\"></a>\n",
        "\n",
        "In this notebook, we performed text classification on a subset of the 20 Newsgroups dataset using various feature extraction techniques and classifiers. We used CountVectorizer, TfidfVectorizer, Word2Vec, and Doc2Vec for feature extraction, and MultinomialNB, LogisticRegression, SVC, and DecisionTreeClassifier as classifiers.\n",
        "\n",
        "We defined a pipeline with placeholders for the vectorizer and classifier, and then performed a grid search to find the best combination of parameters. The grid search results were saved to a text file and printed in a tabular format, including the best score and best parameter set.\n",
        "\n",
        "This analysis demonstrates the process of text classification using different feature extraction techniques and classifiers, and highlights the importance of hyperparameter tuning through grid search to find the optimal model configuration.\n",
        "\n",
        "For further improvement, you can consider exploring additional feature extraction techniques, preprocessing steps (e.g., stemming, lemmatization), and other classifiers or ensemble methods. Additionally, you can experiment with different subsets of the 20 Newsgroups dataset or apply this approach to other text classification tasks."
      ],
      "metadata": {
        "id": "42jxoPJH7jb8"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}