{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of Financial News Headlines\n",
        "\n",
        "Author: Mohamed Oussama NAJI\n",
        "\n",
        "Date: March 27, 2024"
      ],
      "metadata": {
        "id": "khd3-YRpHy-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "1. [Introduction](#introduction)\n",
        "2. [Dataset](#dataset)\n",
        "3. [Data Loading](#data-loading)\n",
        "4. [Data Exploration](#data-exploration)\n",
        "5. [Data Cleaning](#data-cleaning)\n",
        "6. [SMOTE (Imbalanced Dataset)](#smote)\n",
        "7. [Bag-of-Words (BoW) Model](#bow-model)\n",
        "8. [TF-IDF Model](#tfidf-model)\n",
        "9. [Train-Test Split](#train-test-split)\n",
        "10. [Classification Algorithms](#classification-algorithms)\n",
        "    - [LightGBM](#lightgbm)\n",
        "    - [Logistic Regression](#logistic-regression)\n",
        "11. [Confusion Matrices](#confusion-matrices)\n",
        "12. [Conclusion](#conclusion)"
      ],
      "metadata": {
        "id": "cDc3NLC3H2-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction <a id=\"introduction\"></a>\n",
        "\n",
        "Sentiment analysis is a powerful technique used to determine the sentiment or emotional tone of a piece of text. In this notebook, we will perform sentiment analysis on financial news headlines to classify them as positive, negative, or neutral.\n",
        "\n",
        "We will explore various steps involved in the sentiment analysis pipeline, including data loading, data cleaning, feature extraction using Bag-of-Words (BoW) and TF-IDF models, and classification using LightGBM and Logistic Regression algorithms. We will also handle the imbalanced dataset using the SMOTE technique and evaluate the performance of the models using confusion matrices.\n"
      ],
      "metadata": {
        "id": "a4w8TY3xH_Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset <a id=\"dataset\"></a>\n",
        "\n",
        "The dataset used in this notebook contains financial news headlines along with their sentiment labels. It can be downloaded from the following URL:\n",
        "https://raw.githubusercontent.com/subashgandyer/datasets/main/financial_news_headlines_sentiment.csv\n"
      ],
      "metadata": {
        "id": "kufLq39hICXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/subashgandyer/datasets/main/financial_news_headlines_sentiment.csv'\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open('financial_news_headlines_sentiment.csv', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Dataset downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the dataset. Status code:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "KiwcwlsfIFt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading <a id=\"data-loading\"></a>"
      ],
      "metadata": {
        "id": "4WniQYYfIHHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "h_cols = ['sentiment', 'headline']\n",
        "news_headlines_df = pd.read_csv('financial_news_headlines_sentiment.csv', sep=',', names=h_cols, encoding='latin-1')\n"
      ],
      "metadata": {
        "id": "VZqyQfY5IIwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration <a id=\"data-exploration\"></a>"
      ],
      "metadata": {
        "id": "2srOtaGrIKS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape : \", news_headlines_df.shape)\n",
        "print(news_headlines_df.head())\n",
        "print(news_headlines_df['sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "g2mX-FC3ILp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning <a id=\"data-cleaning\"></a>"
      ],
      "metadata": {
        "id": "kudwSUGFINV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if news_headlines_df.isnull().sum().any():\n",
        "    news_headlines_df = news_headlines_df.dropna()\n",
        "\n",
        "news_headlines_df = news_headlines_df.drop_duplicates()\n",
        "news_headlines_df['headline'] = news_headlines_df['headline'].str.replace('[^\\w\\s]','')\n",
        "news_headlines_df['headline'] = news_headlines_df['headline'].str.lower()\n",
        "\n",
        "print(\"shape : \", news_headlines_df.shape)\n",
        "print(news_headlines_df.head())\n",
        "print(news_headlines_df['sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "0QXhSkHzIPnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE (Imbalanced Dataset) <a id=\"smote\"></a>"
      ],
      "metadata": {
        "id": "9bo9o0VsISWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "X = news_headlines_df.headline\n",
        "y = news_headlines_df.sentiment\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "train_data_vectorizer = vectorizer.fit_transform(train_data)\n",
        "test_data_vectorizer = vectorizer.transform(test_data)\n",
        "\n",
        "sm = SMOTE(random_state=42)\n",
        "train_data_res, train_labels_res = sm.fit_resample(train_data_vectorizer, train_labels)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(train_data_res, train_labels_res)\n",
        "\n",
        "predicted_labels = model.predict(test_data_vectorizer)\n",
        "print(classification_report(test_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "hIehX1xpIVGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-Words (BoW) Model <a id=\"bow-model\"></a>"
      ],
      "metadata": {
        "id": "Dh2fUbEeIWpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text_data = news_headlines_df['headline'].values\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "woJxkkpnIXtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Model <a id=\"tfidf-model\"></a>"
      ],
      "metadata": {
        "id": "zQOl9f58IY9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "text_data = news_headlines_df['headline'].values\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "Lr1KVZaOIaGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split <a id=\"train-test-split\"></a>"
      ],
      "metadata": {
        "id": "EfEcoaBUIcBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = news_headlines_df.headline\n",
        "y = news_headlines_df.sentiment\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "xS3Yvn2JIdHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Algorithms <a id=\"classification-algorithms\"></a>\n",
        "\n",
        "### LightGBM <a id=\"lightgbm\"></a>"
      ],
      "metadata": {
        "id": "9ZQSpQBZIfLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "text_data = news_headlines_df['headline'].values\n",
        "target_labels = news_headlines_df['sentiment'].values\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_target_labels = label_encoder.fit_transform(target_labels)\n",
        "\n",
        "vectorizers = {\n",
        "    'BoW': CountVectorizer(),\n",
        "    'TF-IDF': TfidfVectorizer()\n",
        "}\n",
        "\n",
        "predicted_labels_lgb = {\n",
        "    'BoW': {},\n",
        "    'TF-IDF': {}\n",
        "}\n",
        "\n",
        "for vectorizer_name, vectorizer in vectorizers.items():\n",
        "    text_features = vectorizer.fit_transform(text_data).astype(np.float32)\n",
        "    train_data, test_data, train_labels, test_labels = train_test_split(text_features, encoded_target_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_data_lgb = lgb.Dataset(train_data, label=train_labels)\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': len(np.unique(encoded_target_labels)),\n",
        "        'metric': 'multi_logloss',\n",
        "        'verbose': -1\n",
        "    }\n",
        "    classifier_lgb = lgb.train(params, train_data_lgb, 100)\n",
        "    predicted_labels_lgb[vectorizer_name] = np.argmax(classifier_lgb.predict(test_data), axis=1)\n"
      ],
      "metadata": {
        "id": "Zjkjfhu9IlAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression <a id=\"logistic-regression\"></a>"
      ],
      "metadata": {
        "id": "GBxgd_S-Inj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "predicted_labels_lr = {\n",
        "    'BoW': {},\n",
        "    'TF-IDF': {}\n",
        "}\n",
        "\n",
        "for vectorizer_name, vectorizer in vectorizers.items():\n",
        "    text_features = vectorizer.fit_transform(text_data)\n",
        "    train_data, test_data, train_labels, test_labels = train_test_split(text_features, encoded_target_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    classifier_lr = LogisticRegression(max_iter=1000)\n",
        "    classifier_lr.fit(train_data, train_labels)\n",
        "    predicted_labels_lr[vectorizer_name] = classifier_lr.predict(test_data)"
      ],
      "metadata": {
        "id": "dEgsXbZUIpJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrices <a id=\"confusion-matrices\"></a>"
      ],
      "metadata": {
        "id": "hX02ZJeEIq9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_target_labels = label_encoder.fit_transform(target_labels)\n",
        "\n",
        "_, encoded_test_labels = train_test_split(encoded_target_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "for vectorizer_name in ['BoW', 'TF-IDF']:\n",
        "    for classifier_name in ['Logistic Regression', 'LightGBM']:\n",
        "        if classifier_name == 'Logistic Regression':\n",
        "            predicted_labels = predicted_labels_lr[vectorizer_name]\n",
        "        else:\n",
        "            predicted_labels = predicted_labels_lgb[vectorizer_name]\n",
        "\n",
        "        encoded_test_labels = np.array(encoded_test_labels)\n",
        "        predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "        conf_matrix = confusion_matrix(encoded_test_labels, predicted_labels)\n",
        "        plt.figure(figsize=(10,7))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "        plt.title(f'Confusion Matrix for {classifier_name} with {vectorizer_name}')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "nTzsNWR4Is4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion <a id=\"conclusion\"></a>\n",
        "\n",
        "In this notebook, we performed sentiment analysis on financial news headlines using various techniques and algorithms. We started by downloading and loading the dataset, followed by data exploration and cleaning.\n",
        "\n",
        "To handle the imbalanced dataset, we applied the SMOTE technique to oversample the minority class. We then extracted features from the text data using Bag-of-Words (BoW) and TF-IDF models.\n",
        "\n",
        "Next, we trained and evaluated two classification algorithms: LightGBM and Logistic Regression. We used the BoW and TF-IDF features as input to these algorithms and made predictions on the test set.\n",
        "\n",
        "Finally, we generated confusion matrices to assess the performance of each classifier-vectorizer combination. The confusion matrices provide insights into the true positive, true negative, false positive, and false negative predictions made by the models.\n",
        "\n",
        "Sentiment analysis of financial news headlines can be valuable for various applications, such as market trend analysis, investment decision-making, and risk assessment. The techniques and algorithms demonstrated in this notebook can be further improved and customized based on specific requirements and domain knowledge.\n",
        "\n",
        "For future work, we can explore other feature extraction techniques, experiment with different classification algorithms, and fine-tune the hyperparameters to enhance the performance of the sentiment analysis models."
      ],
      "metadata": {
        "id": "odRu19THIu7t"
      }
    }
  ]
}